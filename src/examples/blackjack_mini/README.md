# Карточная игра "Blackjack"
Применение методов `Monte Carlo`и `Sarsa(λ)` на примере карточной игры "Blackjack" с упрощенными правилами.

## Правила

+ Игра играется с бесконечной колодой (т.е. карты выбираются «с возвратом»)
+ Каждая карта, выбираемая из колоды, имеет стоимость от 1 до 10 (вероятность вытащить
любое значение – 1/10), а также либо красный цвет (с вероятностью 1/3), либо черный (с
вероятностью 2/3)
+ В колоде нет тузов и карт с персонажами
+ В начале игры сдающий и игрок достают по одной черной карте и кладут ее лицом вверх
+ Во время каждого хода игрок может выполнить одно из двух действий, которые мы будем
обозначать кодовыми названиями «еще» и «хватит»
+ Если игрок выполняет действие «еще», то он берет одну карту из колоды
+ Если игрок выполняет действие «хватит», то ход переходит сдающему
+ Сумма очков вычисляется следующим образом: значения черных карт прибавляются, а
красных – вычитаются из общей суммы
+ Если сумма очков превышает 21 или становится отрицательной, то игрок проигрывает
(получает награду -1) и игра заканчивается
+ Сдающий всегда набирает карты, пока не получит сумму очков, равную или превосходящую 17. Если сдающий набирает больше 21 или меньше 0 очков, то он проигрывает, а игрок
выигрывает (получает награду +1). В противном случае победа определяется по сумме
очков: у кого больше, тот и выиграл. Ничья приносит нулевую награду

## Результаты

### `Monte Carlo` (500 000 эпизодов)

+ График функции состояния:
![state function graph](https://github.com/Oginsky/reinforcement_learning/raw/main/data/graphs/blackjack_mini/value_function_500k_episodes.png)
---

### `Sarsa(λ)` (10 000 эпизодов)

+ Кривая обучения (λ = 0.0)
![learning curve graph](https://github.com/Oginsky/reinforcement_learning/raw/main/data/graphs/blackjack_mini/mse_episode_lambda=0_10000.png)
---

+ Кривая обучения (λ = 1.0)
![learning curve graph](https://github.com/Oginsky/reinforcement_learning/raw/main/data/graphs/blackjack_mini/mse_episode_lambda=1_10000.png)
