# Задача игрока
*(Пример 4.3. Reinforcement Learning: An Introduction. Richard S. Sutton and Andrew G. Barto)*

Применение алгоритма `итерации по ценностям` для задачи с игрой в монетку. 

## Правила
Игрок может ставить на исходы подбрасывания монеты. Если выпадает орел, то он выигрывает столько долларов, сколько поставил, а если решка, то теряет свою ставку. Игра заканчивается, когда игрок либо выигрывает, достигнув поставленной цели 100 долларов, либо проигрывает все свои
деньги. При каждом подбрасывании игрок должен решить, какую часть своего капитала (целое количество долларов) поставить. Эту задачу можно рассматривать
как эпизодический конечный МППР без обесценивания. Состоянием является капитал игрока, s ∈ {1, 2, …, 99}, а действиями – ставки, a ∈ {0, 1, …, min(s, 100 – s)}.
Вознаграждение равно нулю для всех переходов, кроме тех, на которых игрок достигает своей цели, а в этом случае оно равно +1. Тогда функция ценности состояний дает вероятность выигрыша при старте из каждого состояния. Стратегия отображает величины капитала на ставки. Оптимальная стратегия максимизирует
вероятность достижения цели. Обозначим ph - вероятность выпадения орла.

## Результаты
+ ph = 0.25

**Оценка ценности:**

![state function graph](https://github.com/Oginsky/reinforcement_learning/raw/main/data/graphs/coin/value_function_ph=0.25.png)

**Конечная стратегия:**

![policy graph](https://github.com/Oginsky/reinforcement_learning/raw/main/data/graphs/coin/policy_ph=0.25.png)

---

+ ph = 0.55

**Оценка ценности:**

![state function graph](https://github.com/Oginsky/reinforcement_learning/raw/main/data/graphs/coin/value_function_ph=0.55.png)

**Конечная стратегия:**

![policy graph](https://github.com/Oginsky/reinforcement_learning/raw/main/data/graphs/coin/policy_ph=0.55.png)
